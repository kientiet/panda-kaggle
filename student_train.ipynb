{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GeForce GTX 1080 Ti', ('10.92', 'gigabytes'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "\n",
    "def format_bytes(size):\n",
    "    # 2**10 = 1024\n",
    "    power = 2**10\n",
    "    n = 0\n",
    "    power_labels = {0 : '', 1: 'kilo', 2: 'mega', 3: 'giga', 4: 'tera'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    return \"%.2f\" % size, power_labels[n] + 'bytes'\n",
    "\n",
    "torch.cuda.get_device_name(), format_bytes(torch.cuda.get_device_properties(device).total_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Teacher Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Load from data frame\n",
      "\n",
      "\n",
      ">> The transformation of student is\n",
      "<function apply_policy at 0x7fce6ebaaee0>\n",
      ">> Return trainloader\n",
      ">> Return valloader\n"
     ]
    }
   ],
   "source": [
    "from trainer.distill.ban import BanStudentTrainer\n",
    "model = BanStudentTrainer(1)\n",
    "max_epoches = model.get_max_epoches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-357a4accf8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m checkpoint_callback = ModelCheckpoint(\n\u001b[1;32m      3\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msave_top_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(os.getcwd(), \"checkpoint\", model.model_name)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath = checkpoint_path,\n",
    "    save_top_k = 5,\n",
    "    verbose = True,\n",
    "    monitor = 'kappa_score',\n",
    "    mode = 'max'\n",
    ")\n",
    "\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:GPU available: True, used: True\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "tb_logger = loggers.TensorBoardLogger('logs/', name = model.model_name)\n",
    "trainer = pl.Trainer(checkpoint_callback = checkpoint_callback,\n",
    "                    nb_sanity_val_steps = 0, \n",
    "                    max_epochs = max_epoches, \n",
    "                    gpus = -1, \n",
    "                    logger = tb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    base_momentum: 0.85\n",
       "    dampening: 0\n",
       "    initial_lr: 0.0012\n",
       "    lr: 0.029999968800641534\n",
       "    max_lr: 0.03\n",
       "    max_momentum: 0.9\n",
       "    min_lr: 1.1999999999999999e-05\n",
       "    momentum: 0.8500000520197386\n",
       "    nesterov: False\n",
       "    weight_decay: 5e-05\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.max_lr = 3e-2\n",
    "model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning:\n",
      "    | Name                             | Type              | Params\n",
      "-------------------------------------------------------------------\n",
      "0   | encoder                          | ResNetModel       | 66 M  \n",
      "1   | encoder.encoder                  | Sequential        | 66 M  \n",
      "2   | encoder.encoder.0                | Conv2d            | 9 K   \n",
      "3   | encoder.encoder.1                | BatchNorm2d       | 128   \n",
      "4   | encoder.encoder.2                | ReLU              | 0     \n",
      "5   | encoder.encoder.3                | MaxPool2d         | 0     \n",
      "6   | encoder.encoder.4                | Sequential        | 634 K \n",
      "7   | encoder.encoder.4.0              | Bottleneck        | 206 K \n",
      "8   | encoder.encoder.4.0.conv1        | Conv2d            | 8 K   \n",
      "9   | encoder.encoder.4.0.bn1          | BatchNorm2d       | 256   \n",
      "10  | encoder.encoder.4.0.conv2        | Conv2d            | 147 K \n",
      "11  | encoder.encoder.4.0.bn2          | BatchNorm2d       | 256   \n",
      "12  | encoder.encoder.4.0.conv3        | Conv2d            | 32 K  \n",
      "13  | encoder.encoder.4.0.bn3          | BatchNorm2d       | 512   \n",
      "14  | encoder.encoder.4.0.relu         | ReLU              | 0     \n",
      "15  | encoder.encoder.4.0.downsample   | Sequential        | 16 K  \n",
      "16  | encoder.encoder.4.0.downsample.0 | Conv2d            | 16 K  \n",
      "17  | encoder.encoder.4.0.downsample.1 | BatchNorm2d       | 512   \n",
      "18  | encoder.encoder.4.1              | Bottleneck        | 214 K \n",
      "19  | encoder.encoder.4.1.conv1        | Conv2d            | 32 K  \n",
      "20  | encoder.encoder.4.1.bn1          | BatchNorm2d       | 256   \n",
      "21  | encoder.encoder.4.1.conv2        | Conv2d            | 147 K \n",
      "22  | encoder.encoder.4.1.bn2          | BatchNorm2d       | 256   \n",
      "23  | encoder.encoder.4.1.conv3        | Conv2d            | 32 K  \n",
      "24  | encoder.encoder.4.1.bn3          | BatchNorm2d       | 512   \n",
      "25  | encoder.encoder.4.1.relu         | ReLU              | 0     \n",
      "26  | encoder.encoder.4.2              | Bottleneck        | 214 K \n",
      "27  | encoder.encoder.4.2.conv1        | Conv2d            | 32 K  \n",
      "28  | encoder.encoder.4.2.bn1          | BatchNorm2d       | 256   \n",
      "29  | encoder.encoder.4.2.conv2        | Conv2d            | 147 K \n",
      "30  | encoder.encoder.4.2.bn2          | BatchNorm2d       | 256   \n",
      "31  | encoder.encoder.4.2.conv3        | Conv2d            | 32 K  \n",
      "32  | encoder.encoder.4.2.bn3          | BatchNorm2d       | 512   \n",
      "33  | encoder.encoder.4.2.relu         | ReLU              | 0     \n",
      "34  | encoder.encoder.5                | Sequential        | 3 M   \n",
      "35  | encoder.encoder.5.0              | Bottleneck        | 920 K \n",
      "36  | encoder.encoder.5.0.conv1        | Conv2d            | 65 K  \n",
      "37  | encoder.encoder.5.0.bn1          | BatchNorm2d       | 512   \n",
      "38  | encoder.encoder.5.0.conv2        | Conv2d            | 589 K \n",
      "39  | encoder.encoder.5.0.bn2          | BatchNorm2d       | 512   \n",
      "40  | encoder.encoder.5.0.conv3        | Conv2d            | 131 K \n",
      "41  | encoder.encoder.5.0.bn3          | BatchNorm2d       | 1 K   \n",
      "42  | encoder.encoder.5.0.relu         | ReLU              | 0     \n",
      "43  | encoder.encoder.5.0.downsample   | Sequential        | 132 K \n",
      "44  | encoder.encoder.5.0.downsample.0 | Conv2d            | 131 K \n",
      "45  | encoder.encoder.5.0.downsample.1 | BatchNorm2d       | 1 K   \n",
      "46  | encoder.encoder.5.1              | Bottleneck        | 854 K \n",
      "47  | encoder.encoder.5.1.conv1        | Conv2d            | 131 K \n",
      "48  | encoder.encoder.5.1.bn1          | BatchNorm2d       | 512   \n",
      "49  | encoder.encoder.5.1.conv2        | Conv2d            | 589 K \n",
      "50  | encoder.encoder.5.1.bn2          | BatchNorm2d       | 512   \n",
      "51  | encoder.encoder.5.1.conv3        | Conv2d            | 131 K \n",
      "52  | encoder.encoder.5.1.bn3          | BatchNorm2d       | 1 K   \n",
      "53  | encoder.encoder.5.1.relu         | ReLU              | 0     \n",
      "54  | encoder.encoder.5.2              | Bottleneck        | 854 K \n",
      "55  | encoder.encoder.5.2.conv1        | Conv2d            | 131 K \n",
      "56  | encoder.encoder.5.2.bn1          | BatchNorm2d       | 512   \n",
      "57  | encoder.encoder.5.2.conv2        | Conv2d            | 589 K \n",
      "58  | encoder.encoder.5.2.bn2          | BatchNorm2d       | 512   \n",
      "59  | encoder.encoder.5.2.conv3        | Conv2d            | 131 K \n",
      "60  | encoder.encoder.5.2.bn3          | BatchNorm2d       | 1 K   \n",
      "61  | encoder.encoder.5.2.relu         | ReLU              | 0     \n",
      "62  | encoder.encoder.5.3              | Bottleneck        | 854 K \n",
      "63  | encoder.encoder.5.3.conv1        | Conv2d            | 131 K \n",
      "64  | encoder.encoder.5.3.bn1          | BatchNorm2d       | 512   \n",
      "65  | encoder.encoder.5.3.conv2        | Conv2d            | 589 K \n",
      "66  | encoder.encoder.5.3.bn2          | BatchNorm2d       | 512   \n",
      "67  | encoder.encoder.5.3.conv3        | Conv2d            | 131 K \n",
      "68  | encoder.encoder.5.3.bn3          | BatchNorm2d       | 1 K   \n",
      "69  | encoder.encoder.5.3.relu         | ReLU              | 0     \n",
      "70  | encoder.encoder.6                | Sequential        | 20 M  \n",
      "71  | encoder.encoder.6.0              | Bottleneck        | 3 M   \n",
      "72  | encoder.encoder.6.0.conv1        | Conv2d            | 262 K \n",
      "73  | encoder.encoder.6.0.bn1          | BatchNorm2d       | 1 K   \n",
      "74  | encoder.encoder.6.0.conv2        | Conv2d            | 2 M   \n",
      "75  | encoder.encoder.6.0.bn2          | BatchNorm2d       | 1 K   \n",
      "76  | encoder.encoder.6.0.conv3        | Conv2d            | 524 K \n",
      "77  | encoder.encoder.6.0.bn3          | BatchNorm2d       | 2 K   \n",
      "78  | encoder.encoder.6.0.relu         | ReLU              | 0     \n",
      "79  | encoder.encoder.6.0.downsample   | Sequential        | 526 K \n",
      "80  | encoder.encoder.6.0.downsample.0 | Conv2d            | 524 K \n",
      "81  | encoder.encoder.6.0.downsample.1 | BatchNorm2d       | 2 K   \n",
      "82  | encoder.encoder.6.1              | Bottleneck        | 3 M   \n",
      "83  | encoder.encoder.6.1.conv1        | Conv2d            | 524 K \n",
      "84  | encoder.encoder.6.1.bn1          | BatchNorm2d       | 1 K   \n",
      "85  | encoder.encoder.6.1.conv2        | Conv2d            | 2 M   \n",
      "86  | encoder.encoder.6.1.bn2          | BatchNorm2d       | 1 K   \n",
      "87  | encoder.encoder.6.1.conv3        | Conv2d            | 524 K \n",
      "88  | encoder.encoder.6.1.bn3          | BatchNorm2d       | 2 K   \n",
      "89  | encoder.encoder.6.1.relu         | ReLU              | 0     \n",
      "90  | encoder.encoder.6.2              | Bottleneck        | 3 M   \n",
      "91  | encoder.encoder.6.2.conv1        | Conv2d            | 524 K \n",
      "92  | encoder.encoder.6.2.bn1          | BatchNorm2d       | 1 K   \n",
      "93  | encoder.encoder.6.2.conv2        | Conv2d            | 2 M   \n",
      "94  | encoder.encoder.6.2.bn2          | BatchNorm2d       | 1 K   \n",
      "95  | encoder.encoder.6.2.conv3        | Conv2d            | 524 K \n",
      "96  | encoder.encoder.6.2.bn3          | BatchNorm2d       | 2 K   \n",
      "97  | encoder.encoder.6.2.relu         | ReLU              | 0     \n",
      "98  | encoder.encoder.6.3              | Bottleneck        | 3 M   \n",
      "99  | encoder.encoder.6.3.conv1        | Conv2d            | 524 K \n",
      "100 | encoder.encoder.6.3.bn1          | BatchNorm2d       | 1 K   \n",
      "101 | encoder.encoder.6.3.conv2        | Conv2d            | 2 M   \n",
      "102 | encoder.encoder.6.3.bn2          | BatchNorm2d       | 1 K   \n",
      "103 | encoder.encoder.6.3.conv3        | Conv2d            | 524 K \n",
      "104 | encoder.encoder.6.3.bn3          | BatchNorm2d       | 2 K   \n",
      "105 | encoder.encoder.6.3.relu         | ReLU              | 0     \n",
      "106 | encoder.encoder.6.4              | Bottleneck        | 3 M   \n",
      "107 | encoder.encoder.6.4.conv1        | Conv2d            | 524 K \n",
      "108 | encoder.encoder.6.4.bn1          | BatchNorm2d       | 1 K   \n",
      "109 | encoder.encoder.6.4.conv2        | Conv2d            | 2 M   \n",
      "110 | encoder.encoder.6.4.bn2          | BatchNorm2d       | 1 K   \n",
      "111 | encoder.encoder.6.4.conv3        | Conv2d            | 524 K \n",
      "112 | encoder.encoder.6.4.bn3          | BatchNorm2d       | 2 K   \n",
      "113 | encoder.encoder.6.4.relu         | ReLU              | 0     \n",
      "114 | encoder.encoder.6.5              | Bottleneck        | 3 M   \n",
      "115 | encoder.encoder.6.5.conv1        | Conv2d            | 524 K \n",
      "116 | encoder.encoder.6.5.bn1          | BatchNorm2d       | 1 K   \n",
      "117 | encoder.encoder.6.5.conv2        | Conv2d            | 2 M   \n",
      "118 | encoder.encoder.6.5.bn2          | BatchNorm2d       | 1 K   \n",
      "119 | encoder.encoder.6.5.conv3        | Conv2d            | 524 K \n",
      "120 | encoder.encoder.6.5.bn3          | BatchNorm2d       | 2 K   \n",
      "121 | encoder.encoder.6.5.relu         | ReLU              | 0     \n",
      "122 | encoder.encoder.7                | Sequential        | 41 M  \n",
      "123 | encoder.encoder.7.0              | Bottleneck        | 14 M  \n",
      "124 | encoder.encoder.7.0.conv1        | Conv2d            | 1 M   \n",
      "125 | encoder.encoder.7.0.bn1          | BatchNorm2d       | 2 K   \n",
      "126 | encoder.encoder.7.0.conv2        | Conv2d            | 9 M   \n",
      "127 | encoder.encoder.7.0.bn2          | BatchNorm2d       | 2 K   \n",
      "128 | encoder.encoder.7.0.conv3        | Conv2d            | 2 M   \n",
      "129 | encoder.encoder.7.0.bn3          | BatchNorm2d       | 4 K   \n",
      "130 | encoder.encoder.7.0.relu         | ReLU              | 0     \n",
      "131 | encoder.encoder.7.0.downsample   | Sequential        | 2 M   \n",
      "132 | encoder.encoder.7.0.downsample.0 | Conv2d            | 2 M   \n",
      "133 | encoder.encoder.7.0.downsample.1 | BatchNorm2d       | 4 K   \n",
      "134 | encoder.encoder.7.1              | Bottleneck        | 13 M  \n",
      "135 | encoder.encoder.7.1.conv1        | Conv2d            | 2 M   \n",
      "136 | encoder.encoder.7.1.bn1          | BatchNorm2d       | 2 K   \n",
      "137 | encoder.encoder.7.1.conv2        | Conv2d            | 9 M   \n",
      "138 | encoder.encoder.7.1.bn2          | BatchNorm2d       | 2 K   \n",
      "139 | encoder.encoder.7.1.conv3        | Conv2d            | 2 M   \n",
      "140 | encoder.encoder.7.1.bn3          | BatchNorm2d       | 4 K   \n",
      "141 | encoder.encoder.7.1.relu         | ReLU              | 0     \n",
      "142 | encoder.encoder.7.2              | Bottleneck        | 13 M  \n",
      "143 | encoder.encoder.7.2.conv1        | Conv2d            | 2 M   \n",
      "144 | encoder.encoder.7.2.bn1          | BatchNorm2d       | 2 K   \n",
      "145 | encoder.encoder.7.2.conv2        | Conv2d            | 9 M   \n",
      "146 | encoder.encoder.7.2.bn2          | BatchNorm2d       | 2 K   \n",
      "147 | encoder.encoder.7.2.conv3        | Conv2d            | 2 M   \n",
      "148 | encoder.encoder.7.2.bn3          | BatchNorm2d       | 4 K   \n",
      "149 | encoder.encoder.7.2.relu         | ReLU              | 0     \n",
      "150 | encoder.encoder.8                | Dropout           | 0     \n",
      "151 | project_layer                    | ProjectLayer      | 2 M   \n",
      "152 | project_layer.avg_pooling        | AdaptiveAvgPool2d | 0     \n",
      "153 | project_layer.max_pooling        | AdaptiveMaxPool2d | 0     \n",
      "154 | project_layer.head               | Sequential        | 2 M   \n",
      "155 | project_layer.head.0             | Flatten           | 0     \n",
      "156 | project_layer.head.1             | Linear            | 2 M   \n",
      "157 | project_layer.head.2             | Mish              | 0     \n",
      "158 | project_layer.head.3             | BatchNorm1d       | 1 K   \n",
      "159 | project_layer.head.4             | Dropout           | 0     \n",
      "160 | project_layer.head.5             | Linear            | 3 K   \n",
      "161 | loss_func                        | BanDistillLoss    | 0     \n",
      "162 | loss_func.cross_entropy          | CrossEntropyLoss  | 0     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Return trainloader\n",
      ">> Return valloader\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f035dacea544cbb3cae9a06c913477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kientiet/Documents/My Project/python/panda-kaggle/preprocessing/dataset/load_ban_dataset.py\", line 38, in __getitem__\n    img = transform(img)\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/kientiet/Documents/My Project/python/panda-kaggle/preprocessing/augment/randaug/randaug.py\", line 163, in __call__\n    return self.pil_transformer(pil_img)\nTypeError: pil_transformer() missing 2 required positional arguments: 'level' and 'img_shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-45d4afebefac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no-cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_core_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;31m# run epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         for batch_idx, (batch, is_last_batch) in self.profiler.profile_iterable(\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_with_is_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_train_batch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         ):\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/profiler/profilers.py\u001b[0m in \u001b[0;36mprofile_iterable\u001b[0;34m(self, iterable, action_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36m_with_is_last\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    798\u001b[0m     See `https://stackoverflow.com/a/1630350 <https://stackoverflow.com/a/1630350>`_\"\"\"\n\u001b[1;32m    799\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m     \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;31m# yield last and has next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/kientiet/Documents/My Project/python/panda-kaggle/preprocessing/dataset/load_ban_dataset.py\", line 38, in __getitem__\n    img = transform(img)\n  File \"/home/kientiet/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/kientiet/Documents/My Project/python/panda-kaggle/preprocessing/augment/randaug/randaug.py\", line 163, in __call__\n    return self.pil_transformer(pil_img)\nTypeError: pil_transformer() missing 2 required positional arguments: 'level' and 'img_shape'\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
